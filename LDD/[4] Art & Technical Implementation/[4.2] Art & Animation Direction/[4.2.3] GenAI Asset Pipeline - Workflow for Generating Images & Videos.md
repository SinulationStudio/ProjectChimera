<!-- Filename: LDD/[4] Art & Technical Implementation/[4.2] Art & Animation Direction/[4.2.3] GenAI Asset Pipeline - Workflow for Generating Images & Videos.md -->

# [4.2.3] GenAI Asset Pipeline - Workflow for Generating FMV Clips

This document outlines the planned workflow for creating the game's visual assets using a suite of Generative AI tools. The goal is to establish a consistent, efficient pipeline for producing a large library of short, high-quality `.webm` video files that will function as the game's core visual engine.

## Core Principle: A Video-First, Iterative Approach
The primary output of this pipeline is not static images, but a comprehensive library of short video clips. We will use the best specialized tool for each task and iterate extensively to achieve the desired quality and consistency.

---

## Stage 1: Base Character & Environment Generation

*   **Primary Tool:** High-fidelity image generation models (e.g., Illustrious, Stable Diffusion with custom LoRAs, Midjourney).
*   **Workflow:**
    1.  **Character Definition:** Before any video is generated, we will create a definitive, high-detail "character sheet" image for each key NPC (e.g., Analyst Sarah). This image will be used as a primary reference for all future generation.
    2.  **Character Consistency:** To ensure characters look consistent, we will train a character-specific **LoRA (Low-Rank Adaptation)** on a set of curated base images. This is a critical step for a believable FMV experience.
    3.  **Environment Definition:** A set of master background images for key locations (e.g., The Hub, QA Testing Room) will be generated.

## Stage 2: FMV Clip Generation (The Core Asset Loop)

This is the most critical stage of the pipeline. The goal is to create a complete library of video clips corresponding to every possible game state and action.

*   **Primary Tool:** Image-to-Video or Text-to-Video AI models (e.g., WAN 2.1, Runway Gen-2, Pika Labs).
*   **Workflow:**
    1.  **Source Image & LoRA:** A high-quality static image (from Stage 1) and the character's LoRA will be used as the starting point.
    2.  **Action-Based Prompting:** A clear, direct prompt will describe the desired motion for a specific game action. The prompt will be tied directly to a card or event in the LDD.
        *   *Example Prompt for `Sarah_Neutral_Loop.webm`*: "Using the Sarah LoRA, show her from the chest up in the QA Testing Room. She should have a neutral expression, breathing slowly, occasionally blinking. Loopable 3-5 second video."
        *   *Example Prompt for `Player_Action_KissNeck.webm`*: "Using the Sarah LoRA, from a first-person POV, show her reacting to a kiss on the neck. A slight shiver, a soft gasp, eyes closing for a moment. A 2-3 second, non-looping video."
    3.  **File Naming Convention:** A strict file naming convention is mandatory for organization. The proposed format is: `[Character]_[State/Action]_[Descriptor].webm`. (e.g., `Sarah_Neutral_Loop.webm`, `Sarah_Reaction_Pleasure_High.webm`).
    4.  **Post-Processing:** Generated video clips will be edited in standard video software (e.g., DaVinci Resolve, Premiere) to trim, color correct, and ensure seamless looping where necessary.

## Stage 3: Godot Integration

*   **Engine:** Godot Engine.
*   **Workflow:**
    1.  All finalized video clips will be converted to `.webm` and imported into the Godot project under a structured `assets/videos/` directory.
    2.  Godot's `VideoStreamPlayer` node, controlled by a central state machine script, will be used to call and play the appropriate video file based on game logic (player input, NPC decisions, state changes).

This pipeline transforms the asset creation process into a video production workflow, enabling a solo developer to create a visually rich, FMV-based game.

---
| | [â–² Return to Table of Contents](../../README.md) | |
| :--- | :---: | ---: |